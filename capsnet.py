# -*- coding: utf-8 -*-
"""CapsNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/MahdiyarMM/Colab/blob/master/CapsNet.ipynb
"""

from __future__ import division, print_function, unicode_literals

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import VGG19
from tensorflow.keras.layers import Conv2D,ReLU
from tensorflow.nn import conv2d as convtensor
# restart every graph
tf.reset_default_graph()

# random seed (for same results)
np.random.seed(42)
tf.set_random_seed(42)


def load_fashionmnist():
    # the data, shuffled and split between train and test sets
    from keras.datasets import fashion_mnist
    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.
    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.

    return (x_train, y_train), (x_test, y_test)
def load_mnist():
    # the data, shuffled and split between train and test sets
    from keras.datasets import mnist
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.
    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.

    return (x_train, y_train), (x_test, y_test)

def load_cifar10():
    # the data, shuffled and split between train and test sets
    from keras.datasets import cifar10
    (x_train, y_train), (x_test, y_test) = cifar10.load_data()
    x_train = x_train.reshape(-1, 32, 32, 3).astype('float32') / 255.
    x_test = x_test.reshape(-1, 32, 32, 3).astype('float32') / 255.
    #    x_train = (x_train[:,:,:,1] + x_train[:,:,:,2] + x_train[:,:,:,0])/3
    #    x_test = (x_test[:,:,:,1] + x_test[:,:,:,2] + x_test[:,:,:,0])/3
    return (x_train, y_train), (x_test, y_test)


(x_train, y_train), (x_test, y_test) = load_fashionmnist()


def batchload(batch_size, x_train, y_trian):
    randindex = np.random.choice(x_train.shape[0], batch_size)
    X_batch = []
    Y_batch = []
    for index in randindex:
        xtrind = x_train[index].reshape((-1))
        ytrind = y_train[index].reshape((-1))
        X_batch = np.append(X_batch, xtrind)
        Y_batch = np.append(Y_batch, ytrind)
    X_batch = X_batch.reshape((batch_size, -1))
    Y_batch = Y_batch.reshape((batch_size))
    return X_batch, Y_batch


def batchloadtest(batch_size, x_train, y_trian):
    randindex = np.random.choice(x_test.shape[0], batch_size)
    X_batch = []
    Y_batch = []
    for index in randindex:
        xtrind = x_test[index].reshape((-1))
        ytrind = y_test[index].reshape((-1))
        X_batch = np.append(X_batch, xtrind)
        Y_batch = np.append(Y_batch, ytrind)
    X_batch = X_batch.reshape((batch_size, -1))
    Y_batch = Y_batch.reshape((batch_size))
    return X_batch, Y_batch


def batchloadtestnr(batch_size, x_train, y_trian,ii):
    X_batch = []
    Y_batch = []
    for index in range(ii*batch_size,(ii+1)*batch_size):
        xtrind = x_test[index].reshape((-1))
        ytrind = y_test[index].reshape((-1))
        X_batch = np.append(X_batch, xtrind)
        Y_batch = np.append(Y_batch, ytrind)
    X_batch = X_batch.reshape((batch_size, -1))
    Y_batch = Y_batch.reshape((batch_size))
    return X_batch, Y_batch


# load mnist
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets("/tmp/data/")

##take a look at some MNIST samples
# n_samples = 5
#
# plt.figure(figsize=(n_samples * 2, 3))
# for index in range(n_samples):
#    plt.subplot(1, n_samples, index + 1)
#    sample_image = mnist.train.images[index].reshape(28, 28)
#    plt.imshow(sample_image, cmap="binary")
#    plt.axis("off")
#
# plt.show()
#
# mnist.train.labels[:n_samples]


# Let's builde a CapsNet
# The input placeholder  (28Ã—28 pixels, 1 color channel = grayscale).
imgh = x_train[0].shape[0]
imgw = x_train[0].shape[1]
imgch = x_train[0].shape[2]

X = tf.placeholder(shape=[None, imgh, imgw, imgch], dtype=tf.float32, name="X")

caps1_n_caps = 100  # 1152 primary capsules
caps1_n_dims = 8

# First -> Convoloutional layers
# Pre trained layers

# 28x28 -> (after one conve)20x20 -> (after two conves)12x12 -> Strides(2) ->6x6





mask = tf.Variable(np.ones((9,9,1,256)).astype("float32"),name="mask1",trainable = False)
filters_conv1 = tf.Variable(np.random.normal(scale=0.01,size=(9,9,1,256)).astype("float32"),name="filters_conv1")
filter1 = tf.math.multiply(mask,filters_conv1)
conv1 = convtensor(X,filter1 ,strides=1,padding="VALID")
conv1maskedrelu = ReLU()(conv1)


mask2 = tf.Variable(np.ones((9,9,256,256)).astype("float32"),name="mask2",trainable = False)
filters_conv2 = tf.Variable(np.random.normal(scale=0.01,size=(9,9,256,256)).astype("float32"),name="filters_conv2")
filter2 = tf.math.multiply(mask2,filters_conv2)
conv2 = convtensor(conv1maskedrelu,filter2,strides=2,padding="VALID")
conv2maskedrelu = ReLU()(conv2)


'''
mask = np.ones((9,9,1,256))
mask_tensor = tf.Variable(mask.astype("float32"),name="mask_val")
filters_conv1 = tf.Variable(np.random.normal(scale=0.01,size=(9,9,1,256)).astype("float32"),name="filters_conv1")
masked_filter = tf.math.multiply(mask_tensor,filters_conv1)
conv1 = convtensor(X,masked_filter,strides=1,padding="VALID")
conv1maskedrelu = ReLU()(conv1)

# offset_val2 = np.zeros((50, 6, 6, 256))
# offset2 = tf.Variable(offset_val2.astype("float32"),name="offset_val2")
filters_conv2 = tf.Variable(np.random.normal(scale=0.01,size=(9,9,256,256)).astype("float32"),name="filters_conv2")
conv2 = convtensor(conv1maskedrelu,filters_conv2,strides=2,padding="VALID")
# compare2 = tf.math.greater(conv2,offset2)
# mask2 = tf.cast(compare2, tf.float32)
# conv2masked = tf.math.multiply(mask2,conv2)
conv2maskedrelu = ReLU()(conv2)
'''

nonzero = tf.math.count_nonzero(conv1maskedrelu)


flatten_cnn = tf.layers.flatten(conv2maskedrelu, name="flatten_CNN")
nonzero1 = tf.math.count_nonzero(flatten_cnn)
dense_1 = tf.layers.dense(flatten_cnn, caps1_n_caps * caps1_n_dims, activation=tf.nn.relu)
# flaten the output to 6*6*32 (6x6 pic and 32 caps1_n_maps) What???
caps1_raw = tf.reshape(dense_1, [-1, caps1_n_caps, caps1_n_dims],
                       name="caps1_raw")


# Let's do squash! :D
# the length of the vector should be in the [0,1] range

def squash(s, axis=-1, epsilon=1e-7, name=None):
    with tf.name_scope(name, default_name="squash"):
        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,
                                     keep_dims=True)
        safe_norm = tf.sqrt(squared_norm + epsilon)
        squash_factor = squared_norm / (1. + squared_norm)
        unit_vector = s / safe_norm
        return squash_factor * unit_vector


# the primary capsule output
caps1_output = squash(caps1_raw, name="caps1_output")

# Now let's move on to the Digit capsule
# number of the classes -> 10
# the dimention is 16D
caps2_n_caps = 10
caps2_n_dims = 16

# Computing the output of the Digit Caps layer by a massive matrix multipication
# creat random weights around zero
init_sigma = 0.1

W_init = tf.random_normal(
    shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),
    stddev=init_sigma, dtype=tf.float32, name="W_init")
W = tf.Variable(W_init, name="W")

batch_size = tf.shape(X)[0]
W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name="W_tiled")

# the first caps output
caps1_output_expanded = tf.expand_dims(caps1_output, -1,
                                       name="caps1_output_expanded")
caps1_output_tile = tf.expand_dims(caps1_output_expanded, 2,
                                   name="caps1_output_tile")
caps1_output_tiled = tf.tile(caps1_output_tile, [1, 1, caps2_n_caps, 1, 1],
                             name="caps1_output_tiled")

# multiply first caps output to the weights to get prediction results
caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled,
                            name="caps2_predicted")
nonzero2 = tf.math.count_nonzero(caps2_predicted)
# the shape of caps2_predicted is (?, 1152, 10, 16, 1) which is a 16 dimentional prediction of 32 dimentional capsules of 6*6 images


# Lets do routing by agreement
# set rowting weights to zero
raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1],
                       dtype=np.float32, name="raw_weights")

# Round 1 of routing
# calculate weights by applying softmax
routing_weights = tf.nn.softmax(raw_weights, dim=2, name="routing_weights")

# apply weights to prediction results
weighted_predictions = tf.multiply(routing_weights, caps2_predicted,
                                   name="weighted_predictions")
weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keep_dims=True,
                             name="weighted_sum")

# Squash it!
caps2_output_round_1 = squash(weighted_sum, axis=-2,
                              name="caps2_output_round_1")

# Round2
# calculate the distance of the prediction to the output vectors
caps2_output_round_1_tiled = tf.tile(
    caps2_output_round_1, [1, caps1_n_caps, 1, 1, 1],
    name="caps2_output_round_1_tiled")

agreement = tf.matmul(caps2_predicted, caps2_output_round_1_tiled,
                      transpose_a=True, name="agreement")

raw_weights_round_2 = tf.add(raw_weights, agreement,
                             name="raw_weights_round_2")

routing_weights_round_2 = tf.nn.softmax(raw_weights_round_2,
                                        dim=2,
                                        name="routing_weights_round_2")
weighted_predictions_round_2 = tf.multiply(routing_weights_round_2,
                                           caps2_predicted,
                                           name="weighted_predictions_round_2")
weighted_sum_round_2 = tf.reduce_sum(weighted_predictions_round_2,
                                     axis=1, keep_dims=True,
                                     name="weighted_sum_round_2")
caps2_output_round_2 = squash(weighted_sum_round_2,
                              axis=-2,
                              name="caps2_output_round_2")

# We should probably add some more rounds


#Round3
#calculate the distance of the prediction to the output vectors
caps2_output_round_2_tiled = tf.tile(
    caps2_output_round_2, [1, caps1_n_caps, 1, 1, 1],
    name="caps2_output_round_2_tiled")


agreement = tf.matmul(caps2_predicted, caps2_output_round_2_tiled,
                      transpose_a=True, name="agreement")

raw_weights_round_3 = tf.add(raw_weights_round_2, agreement,
                             name="raw_weights_round_3")

routing_weights_round_3 = tf.nn.softmax(raw_weights_round_3,
                                        dim=2,
                                        name="routing_weights_round_3")
weighted_predictions_round_3 = tf.multiply(routing_weights_round_3,
                                           caps2_predicted,
                                           name="weighted_predictions_round_3")
weighted_sum_round_3 = tf.reduce_sum(weighted_predictions_round_3,
                                     axis=1, keep_dims=True,
                                     name="weighted_sum_round_3")
caps2_output_round_3 = squash(weighted_sum_round_3,
                              axis=-2,
                              name="caps2_output_round_3")

#We should probably add some more rounds
caps2_output = caps2_output_round_3


# class probability
def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False, name=None):
    with tf.name_scope(name, default_name="safe_norm"):
        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,
                                     keep_dims=keep_dims)
        return tf.sqrt(squared_norm + epsilon)


y_proba = safe_norm(caps2_output, axis=-2, name="y_proba")

# find the class with highest probability
y_proba_argmax = tf.argmax(y_proba, axis=2, name="y_proba")
y_pred = tf.squeeze(y_proba_argmax, axis=[1, 2], name="y_pred")

# it is done! We have the output of the model

# know the training
# let's keep labels in a placeholder
y = tf.placeholder(shape=[None], dtype=tf.int64, name="y")

# compute the margine loss (complictaed equation ;D)
m_plus = 0.9
m_minus = 0.1
lambda_ = 0.5
# one_hot is creating [0,0,0,1,...,0] for every class (so y should be the number of the class)
T = tf.one_hot(y, depth=caps2_n_caps, name="T")

# comparing the norm of the output with labels
caps2_output_norm = safe_norm(caps2_output, axis=-2, keep_dims=True,
                              name="caps2_output_norm")

# I think the number 10 in shape=(-1, 10) is the number of classes
present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm),
                              name="present_error_raw")
present_error = tf.reshape(present_error_raw, shape=(-1, 10),
                           name="present_error")

absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus),
                             name="absent_error_raw")
absent_error = tf.reshape(absent_error_raw, shape=(-1, 10),
                          name="absent_error")

# compute the creepy loss for every instance
L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error,
           name="L")

# the whole margine loss
margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name="margin_loss")

##finally let's reconstruct image to compute second error
# but first, Maksing
mask_with_labels = tf.placeholder_with_default(False, shape=(),
                                               name="mask_with_labels")

reconstruction_targets = tf.cond(mask_with_labels,  # condition
                                 lambda: y,  # if True
                                 lambda: y_pred,  # if False
                                 name="reconstruction_targets")
# creat the mask
reconstruction_mask = tf.one_hot(reconstruction_targets,
                                 depth=caps2_n_caps,
                                 name="reconstruction_mask")

# change the shape of the mask
reconstruction_mask_reshaped = tf.reshape(
    reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1],
    name="reconstruction_mask_reshaped")

# mask the output
caps2_output_masked = tf.multiply(
    caps2_output, reconstruction_mask_reshaped,
    name="caps2_output_masked")

# flaten decoder input

decoder_input = tf.reshape(caps2_output_masked,
                           [-1, caps2_n_caps * caps2_n_dims],
                           name="decoder_input")

# Now let's creat the decoder to reconstruct image based on the masked output

n_hidden1 = 512
n_hidden2 = 1024
n_output = imgh * imgw * imgch

with tf.name_scope("decoder"):
    hidden1 = tf.layers.dense(decoder_input, n_hidden1,
                              activation=tf.nn.relu,
                              name="hidden1")
    hidden2 = tf.layers.dense(hidden1, n_hidden2,
                              activation=tf.nn.relu,
                              name="hidden2")
    decoder_output = tf.layers.dense(hidden2, n_output,
                                     activation=tf.nn.sigmoid,
                                     name="decoder_output")

# Reconstruction loss

X_flat = tf.reshape(X, [-1, n_output], name="X_flat")
squared_difference = tf.square(X_flat - decoder_output,
                               name="squared_difference")
reconstruction_loss = tf.reduce_mean(squared_difference,
                                     name="reconstruction_loss")

# Final loss

alpha = 0.0005

loss = tf.add(margin_loss, alpha * reconstruction_loss, name="loss")

# evaluate models accuracy by conting true classes
correct = tf.equal(y, y_pred, name="correct")
accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")

# now let's creat the optimizer ADAM

optimizer = tf.train.AdamOptimizer()
training_op = optimizer.minimize(loss, name="training_op")

init = tf.global_variables_initializer()
saver = tf.train.Saver()

##Training

n_epochs = 1
batch_size = 50
restore_checkpoint = False

n_iterations_per_epoch = int(mnist.train.num_examples) // batch_size
n_iterations_validation = int(mnist.validation.num_examples) // batch_size

best_loss_val = np.infty
checkpoint_path = "./my_capsule_network"

with tf.Session() as sess:
    if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):
        saver.restore(sess, checkpoint_path)
        assign_op = mask.assign(mask_val)
        sess.run(assign_op)
    else:
        init.run()
    nz_th =[]
    loss_th = []

    for epoch in range(n_epochs):
        
        for iteration in range(1, n_iterations_per_epoch + 1):
            X_batch, y_batch = batchload(batch_size, x_train, y_train)
            # Run the training operation and measure the loss:
            _, loss_train,nozero,sss = sess.run(
                [training_op, loss, nonzero,tf.shape(conv1)],
                feed_dict={X: X_batch.reshape([-1, imgh, imgw, imgch]),
                           y: y_batch,
                           mask_with_labels: True})
            print("\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f} nozero {:.5f}".format(
                iteration, n_iterations_per_epoch,
                iteration * 100 / n_iterations_per_epoch,
                loss_train ,(nozero/(np.prod(sss))*100) ),
                end="")
            nz_th.append(nozero/(np.prod(sss))*100)
            loss_th.append(loss_train)
        # At the end of each epoch,
        # measure the validation loss and accuracy:
        loss_vals = []
        acc_vals = []
        for iteration in range(1, n_iterations_validation + 1):
            X_batch, y_batch = batchloadtest(batch_size, x_train, y_train)
            loss_val, acc_val = sess.run(
                [loss, accuracy],
                feed_dict={X: X_batch.reshape([-1, imgh, imgw, imgch]),
                           y: y_batch})
            loss_vals.append(loss_val)
            acc_vals.append(acc_val)
            print("\rEvaluating the model: {}/{} ({:.1f}%)".format(
                iteration, n_iterations_validation,
                iteration * 100 / n_iterations_validation),
                end=" " * 10)
        loss_val = np.mean(loss_vals)
        acc_val = np.mean(acc_vals)
        print("\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}".format(
            epoch + 1, acc_val * 100, loss_val,
            " (improved)" if loss_val < best_loss_val else ""))

        # And save the model if it improved:
        if loss_val < best_loss_val:
            save_path = saver.save(sess, checkpoint_path)
            best_loss_val = loss_val

#Mask creator
def mask_threshold(f,th):
  maskp = np.greater(f,th).astype("float32")
  maskn = np.logical_not(np.greater(f,-th)).astype("float32")
  return maskn + maskp
  
def mask_filters(f,num):
  mask = np.ones(f.shape)
  m = np.mean(f,axis = 0)
  m = np.mean(m,axis = 0)
  m = np.mean(m,axis = 0)
  ind = np.argsort(m)
  ind = ind[0:num]
  for ii in range(num):
    mask[:,:,:,ind[ii]] = np.zeros((9,9,1))
  return mask



def find_th(f,th):
  return th*np.std(f)  

def mask_e(f,th):
  masked = []
  for ii in range(256):
    masked_kernel = mask_threshold(f[:,:,:,ii],find_th(f[:,:,:,ii],th))
    masked.append(masked_kernel)
  masked = np.array(masked)
  masked = masked.reshape(f.shape)
  return masked


def offset_threshold(w,th):
  return np.ones((50 ,20, 20,256))*th

def offset_stat(w,th):
  mask = []
  for ch in range(50):
    std = np.std(w[ch])
    mask.append(np.ones((20, 20,256))*std*th)
  return np.array(mask)



# Evaluation
n_iterations_test = mnist.test.num_examples // batch_size

with tf.Session() as sess:
    saver.restore(sess, checkpoint_path)
    # w = sess.run([conv1.get_weights()],feed_dict={X: X_batch.reshape([-1, imgh, imgw, imgch]),
    #                    y: y_batch})


    nzshape,nzshape1,nzshape2 = sess.run([tf.shape(conv1),tf.shape(flatten_cnn),tf.shape(caps2_predicted)],feed_dict={X: X_batch.reshape([-1, imgh, imgw, imgch]),
                                            y: y_batch})
    for ii in range(1):

      loss_tests = []
      acc_tests = []
      nz =[]
      nz1 =[]
      nz2 =[]
      conv1 =[]
      
      
      filters = [v for v in tf.global_variables() if v.name == 'filters_conv2:0'][0]
      mask = [v for v in tf.global_variables() if v.name == 'mask2:0'][0]
      f = sess.run(filters)
      mask_val = mask_filters(f, 2*ii)

      assign_op = mask.assign(mask_val)
      sess.run(assign_op)
      print(np.mean(mask_val))
      # filters = [v for v in tf.global_variables() if v.name == 'filters_conv2:0'][0]
      # mask = [v for v in tf.global_variables() if v.name == 'mask2:0'][0]
      # f = sess.run(filters)
      # mask_val = mask_threshold(f, ii*0.001)
      # assign_op = mask.assign(mask_val)
      # sess.run(assign_op)

      '''
      filters = [v for v in tf.global_variables() if v.name == 'filters_conv2:0'][0]
      mask = [v for v in tf.global_variables() if v.name == 'mask2:0'][0]
      f = sess.run(filters)
      mask_val = mask_threshold(f, 0.03 + ii*0.001)
      assign_op = mask.assign(mask_val)
      sess.run(assign_op)
      '''
      for iteration in range(1, n_iterations_test ):
          X_batch, y_batch = batchloadtestnr(batch_size, x_train, y_train,iteration )
          loss_test, acc_test, nozero, nozero1, nozero2,conv = sess.run(
              [loss, accuracy, nonzero,nonzero1,nonzero2,conv2],
              feed_dict={X: X_batch.reshape([-1, imgh, imgw, imgch]),
                          y: y_batch})
          loss_tests.append(loss_test)
          acc_tests.append(acc_test)
          nz.append(nozero)
          nz1.append(nozero1)
          nz2.append(nozero2)
          conv1.append(conv)
          print("\rEvaluating the model: {}/{} ({:.1f}%)".format(
              iteration, n_iterations_test,
              iteration * 100 / n_iterations_test),
              end=" " * 10)
      loss_test = np.mean(loss_tests)
      acc_test = np.mean(acc_tests)

      
      print("\rFinal test accuracy: {:.4f}%  Loss: {:.6f}".format(
          acc_test * 100, loss_test))
      #print(1-np.mean(nz/np.prod(nzshape)))

      



      # wei = [v for v in tf.global_variables() if v.name == 'filters_conv1:0'][0]
      # w = sess.run(wei)
      # print(zero_percent(w))
      # wp = f_p(w,0.005*ii)
      # assign_op = wei.assign(wp)
      # sess.run(assign_op)
      # w2 = sess.run(wei)



      # wei2 = [v for v in tf.global_variables() if v.name == 'conv2/kernel:0'][0]
      # w2 = sess.run(wei2)
      # w2p = f_p(w2,0.005)
      # wei.load(wp,sess)


    # g = sess.graph


   

    # conv1_kernel_val = sess.run([g.get_tensor_by_name('conv1/kernel:0')],feed_dict={X: X_batch.reshape([-1, imgh, imgw, imgch]),
    #                              y: y_batch})
    
    # with tf.variable_scope('conv1') as scope:
    #     tf.get_variable_scope().reuse_variables()
    #     weights = tf.get_variable('weights')

plt.hist(np.ndarray.flatten(f))
plt.xlabel("Filter weights")
plt.ylabel("Frequency")
plt.savefig("f2_hist_f.pdf")
plt.show()

print(np.mean(f))
print(np.std(f))

def batchloadtest(batch_size, x_train, y_trian):
    randindex = np.random.choice(x_test.shape[0], batch_size)
    X_batch = []
    Y_batch = []
    for index in randindex:
        xtrind = x_test[index].reshape((-1))
        ytrind = y_test[index].reshape((-1))
        X_batch = np.append(X_batch, xtrind)
        Y_batch = np.append(Y_batch, ytrind)
    X_batch = X_batch.reshape((batch_size, -1))
    Y_batch = Y_batch.reshape((batch_size))
    return X_batch, Y_batch


def batchloadtestnr(batch_size, x_train, y_trian,ii):
    X_batch = []
    Y_batch = []
    for index in range(ii*batch_size,(ii+1)*batch_size-1):
        xtrind = x_test[index].reshape((-1))
        ytrind = y_test[index].reshape((-1))
        X_batch = np.append(X_batch, xtrind)
        Y_batch = np.append(Y_batch, ytrind)
    X_batch = X_batch.reshape((batch_size, -1))
    Y_batch = Y_batch.reshape((batch_size))
    return X_batch, Y_batch

(x_train, y_train), (x_test, y_test) = load_mnist()
X_batch, Y_batch = batchloadtest(36, x_train, y_train)
ii=1
batch_size = 36
print(np.random.choice(x_test.shape[0], batch_size).shape)

print(len(list(range(ii*batch_size,(ii+1)*batch_size))))

print((list(range(ii*batch_size,(ii+1)*batch_size))))

#Test 
#Mask creator 
def mask_threshold(f,th):
  maskp = np.greater(f,th).astype("float32")
  maskn = np.logical_not(np.greater(f,-th)).astype("float32")
  return maskn + maskp
  
def mask_e(f,th):
  for ii in range(256):
     masked_kernel = mask_threshold
  return np.greater(f,th).astype("float32")


def offset_threshold(w,th):
  return np.ones((50 ,20, 20,256))*th

def offset_stat(w,th):
  mask = []
  for ch in range(50):
    std = np.std(w[ch])
    mask.append(np.ones((20, 20,256))*std*th)
  return np.array(mask)



# Evaluation
n_iterations_test = mnist.test.num_examples // batch_size
mask_val = np.ones((9,9,1,256))
with tf.Session() as sess:
    saver.restore(sess, checkpoint_path)
    # w = sess.run([conv1.get_weights()],feed_dict={X: X_batch.reshape([-1, imgh, imgw, imgch]),
    #                    y: y_batch})


    nzshape,nzshape1,nzshape2 = sess.run([tf.shape(conv1),tf.shape(flatten_cnn),tf.shape(caps2_predicted)],feed_dict={X: X_batch.reshape([-1, imgh, imgw, imgch]),
                                            y: y_batch})
  

    loss_tests = []
    acc_tests = []
    nz =[]
    nz1 =[]
    nz2 =[]
    conv1 =[]
    for iteration in range(1, n_iterations_test + 1):
        filters = [v for v in tf.global_variables() if v.name == 'filters_conv1:0'][0]
        mask = [v for v in tf.global_variables() if v.name == 'mask1:0'][0]
        f = sess.run(filters)
        mask_val = mask_threshold(f,0.01)
        #print(np.mean(mask_val))
        #print(mask_val)
        assign_op = mask.assign(np.zeros((9,9,1,256)))
        sess.run(assign_op)
        mask_new = sess.run(mask)
        print(mask_new)

        X_batch, y_batch = batchloadtest(batch_size, x_train, y_train)
        loss_test, acc_test, nozero, nozero1, nozero2,conv = sess.run(
            [loss, accuracy, nonzero,nonzero1,nonzero2,conv2],
            feed_dict={X: X_batch.reshape([-1, imgh, imgw, imgch]),
                        y: y_batch})
        loss_tests.append(loss_test)
        acc_tests.append(acc_test)
        nz.append(nozero)
        nz1.append(nozero1)
        nz2.append(nozero2)
        conv1.append(conv)
        print("\rEvaluating the model: {}/{} ({:.1f}%)".format(
            iteration, n_iterations_test,
            iteration * 100 / n_iterations_test),
            end=" " * 10)
    loss_test = np.mean(loss_tests)
    acc_test = np.mean(acc_tests)

    
    print("\rFinal test accuracy: {:.4f}%  Loss: {:.6f}".format(
        acc_test * 100, loss_test))
    #print(1-np.mean(nz/np.prod(nzshape)))
    print(np.mean(mask_val))
    print(mask_val)
    



      # wei = [v for v in tf.global_variables() if v.name == 'filters_conv1:0'][0]
      # w = sess.run(wei)
      # print(zero_percent(w))
      # wp = f_p(w,0.005*ii)
      # assign_op = wei.assign(wp)
      # sess.run(assign_op)
      # w2 = sess.run(wei)



      # wei2 = [v for v in tf.global_variables() if v.name == 'conv2/kernel:0'][0]
      # w2 = sess.run(wei2)
      # w2p = f_p(w2,0.005)
      # wei.load(wp,sess)


    # g = sess.graph


   

    # conv1_kernel_val = sess.run([g.get_tensor_by_name('conv1/kernel:0')],feed_dict={X: X_batch.reshape([-1, imgh, imgw, imgch]),
    #                              y: y_batch})
    
    # with tf.variable_scope('conv1') as scope:
    #     tf.get_variable_scope().reuse_variables()
    #     weights = tf.get_variable('weights')

import numpy as np
def mask_threshold(f,th):
  return np.greater(f,th).astype("float32")

print(mask_threshold(np.ones((1,5,5,5))*0.2,np.ones((1,5,5,5))*0.3))

print(nzshape)

print(w[:,:,0,1])
ww = w[:,:,0,0].sort()
print(ww)
# print(sorted(all(w[:,:,0,0])))

for ii in range(10):
  plt.imshow((w[:,:,0,ii]))
  plt.show()

def filter_prun(f,th):
  l = len(f)
  f2 = np.zeros((l,l))
  for ii in range(l):
    for jj in range(l):
      if f[ii,jj] > th or f[ii,jj] < -1*th:
        f2[ii,jj] = f[ii,jj]

  return f2

def f_p(w,th):
  shape = w.shape
  w2 = np.zeros(shape)
  for ii in range(shape[-1]):
    w2[:,:,0,ii] = filter_prun(w[:,:,0,ii],th)
  return w2

def zero_percent(w):
  return (1-np.count_nonzero(w)/(np.prod(w.shape)))*100

print(zero_percent(wp),zero_percent(w2p))

print(w2.shape)
wthth = f_p(w2,0.005)
wth = filter_prun(w2[:,:,0,0],0.005)
print(wth)
print(wthth[:,:,0,0]-wth)
print((1-np.count_nonzero(wth)/(len(wth)*len(wth[0])))*100)

print(conv)

print(np.mean((nz)))
nzp = nz/np.prod(nzshape)
nzp1 = nz1/np.prod(nzshape1)
nzp2 = nz2/np.prod(nzshape2)

# print(nzp)
# print(nzp1)
# print(nzp2)

plt.hist(1-nzp)
plt.xlabel("Percentage of zero values")
plt.ylabel("Frequency")
plt.savefig("nzp_rr_p0.001.pdf")
plt.show()

plt.hist(1-nzp1)
plt.xlabel("Percentage of zero values")
plt.ylabel("Frequency")
plt.savefig("nzp1_rr_p0.001.pdf")
plt.show()
plt.hist(1-nzp2)
plt.xlabel("Percentage of zero values")
plt.ylabel("Frequency")
plt.savefig("nzp2_rr_p0.001.pdf")
plt.show()
print([np.mean(nzp),np.mean(nzp1),np.mean(nzp2)])

tf.global_variables()

#    
##Prediction
# n_samples = 5
#
# sample_images = mnist.test.images[:n_samples].reshape([-1, 28, 28, 1])
#
# with tf.Session() as sess:
#    saver.restore(sess, checkpoint_path)
#    caps2_output_value, decoder_output_value, y_pred_value = sess.run(
#            [caps2_output, decoder_output, y_pred],
#            feed_dict={X: sample_images,
#                       y: np.array([], dtype=np.int64)})
#    
#
##Check the dimention effects
#    
#
# def tweak_pose_parameters(output_vectors, min=-0.5, max=0.5, n_steps=11):
#    steps = np.linspace(min, max, n_steps) # -0.25, -0.15, ..., +0.25
#    pose_parameters = np.arange(caps2_n_dims) # 0, 1, ..., 15
#    tweaks = np.zeros([caps2_n_dims, n_steps, 1, 1, 1, caps2_n_dims, 1])
#    tweaks[pose_parameters, :, 0, 0, 0, pose_parameters, 0] = steps
#    output_vectors_expanded = output_vectors[np.newaxis, np.newaxis]
#    return tweaks + output_vectors_expanded
#
# n_steps = 11
#
# tweaked_vectors = tweak_pose_parameters(caps2_output_value, n_steps=n_steps)
# tweaked_vectors_reshaped = tweaked_vectors.reshape(
#    [-1, 1, caps2_n_caps, caps2_n_dims, 1])
#
#
# tweak_labels = np.tile(mnist.test.labels[:n_samples], caps2_n_dims * n_steps)
#
# with tf.Session() as sess:
#    saver.restore(sess, checkpoint_path)
#    decoder_output_value = sess.run(
#            decoder_output,
#            feed_dict={caps2_output: tweaked_vectors_reshaped,
#                       mask_with_labels: True,
#                       y: tweak_labels})
#    
#
# tweak_reconstructions = decoder_output_value.reshape(
#        [caps2_n_dims, n_steps, n_samples, 28, 28])
#
#
# for dim in range(3):
#    print("Tweaking output dimension #{}".format(dim))
#    plt.figure(figsize=(n_steps / 1.2, n_samples / 1.5))
#    for row in range(n_samples):
#        for col in range(n_steps):
#            plt.subplot(n_samples, n_steps, row * n_steps + col + 1)
#            plt.imshow(tweak_reconstructions[dim, col, row], cmap="binary")
#            plt.axis("off")
#    plt.show()
#    
##Thats it :D 
##Thank you French guy!
#
#
#

files.download("nzp.svg")

from google.colab import drive
drive.mount('/content/drive')

#CNN for MNIST

from __future__ import print_function
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Multiply, Input
from keras.layers import Conv2D, MaxPooling2D, conv
from keras import backend as K
from keras.models import Model
from tensorflow.nn import conv2d as convtensor

batch_size = 128
num_classes = 10
epochs = 12

# input image dimensions
img_rows, img_cols = 28, 28

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

if K.image_data_format() == 'channels_first':
    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)
else:
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

imgin = Input(shape=(28,28,1))
mask = Input(shape=(9,9,1,256))
x = Conv2D(256, kernel_size=(9, 9),activation='relu')(imgin)
x = Conv2D(256, (9, 9),strides=2, activation='relu')(x)
x = Flatten()(x)
x = Dense(100*8, activation='relu')(x)
x = Dropout(0.5)(x)
out = Dense(num_classes, activation='softmax')(x)

model = Model(imgin,out)

model.summary()

model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
print(model.layers)
model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=5,
          verbose=1,
          validation_data=(x_test, y_test))
cov1 = model.layers[0].output

score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

net = Model(model.input,model.layers[0].output)
net2 = Model(model.input,model.layers[1].output)
y = net.predict(x_train[0:1000])
y2 = net2.predict(x_train[0:1000])


print(y2.shape)

z = np.count_nonzero(y)
s = np.sum(z)

z2 = np.count_nonzero(y2)
s2 = np.sum(z2)

print(s)
print(1-s/(1000*20*20*256))

print(s2)
print(1-s2/(1000*6*6*256))